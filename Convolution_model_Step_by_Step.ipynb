{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a61683c-6c32-4b9c-9bdd-c1f74d8021fe",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8903ed-eeb3-407b-a4f7-cb06cbbf7579",
   "metadata": {},
   "source": [
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1fa257-f910-4321-b12e-7d52d686db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure default plot settings for consistency and readability\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)  # Set default plot size\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # Avoid smoothing for image data\n",
    "plt.rcParams['image.cmap'] = 'gray'  # Display images in grayscale by default\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Enable Jupyter-specific commands if running in a Jupyter environment\n",
    "try:\n",
    "    # Load the autoreload extension to automatically reload modules before executing code\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "    # Set autoreload to reload all modules (except those imported with \"from ... import ...\")\n",
    "    get_ipython().run_line_magic('autoreload', '2')\n",
    "except NameError:\n",
    "    # If not in Jupyter, skip loading these extensions\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6bc0ad-df09-4364-a9dc-7a659cc3930d",
   "metadata": {},
   "source": [
    "## 3 - Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6873c58a-3e95-4981-8ef0-be71c0f47cec",
   "metadata": {},
   "source": [
    "### 3.1 - Zero-Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1095ced3-885a-4760-9b21-95b2de39c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pads each image in the dataset X with zeros around the height and width.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): Array of shape (m, n_H, n_W, n_C) representing a batch of m images.\n",
    "        pad (int): Amount of padding to add around each image's height and width.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Padded image array of shape (m, n_H + 2*pad, n_W + 2*pad, n_C).\n",
    "    \"\"\"\n",
    "    # Apply zero-padding around height and width dimensions\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode='constant', constant_values=0)\n",
    "    return X_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69278368-a3b1-4c18-994c-f54c175dc9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_pad.shape = (4, 7, 7, 2)\n",
      "x[1, 1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1, 1] = [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7d5e72a78230>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Generate random input data with shape (4, 3, 3, 2)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "\n",
    "# Apply zero padding with a pad value of 2\n",
    "x_pad = zero_pad(x, 2)\n",
    "\n",
    "# Print shapes of the original and padded arrays\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "print(f\"x_pad.shape = {x_pad.shape}\")\n",
    "\n",
    "# Display a specific element from both the original and padded arrays\n",
    "print(f\"x[1, 1] = {x[1, 1]}\")\n",
    "print(f\"x_pad[1, 1] = {x_pad[1, 1]}\")\n",
    "\n",
    "# Visualize the first image from both original and padded arrays\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0, :, :, 0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0, :, :, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72072ecd-6b59-454a-8eeb-353f28124c85",
   "metadata": {},
   "source": [
    "### 3.2 - Single step of convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0a38d54-e70c-48e5-91ef-939c59356a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Perform a single convolutional step by applying filter W and bias b on a slice of input data.\n",
    "    \n",
    "    Parameters:\n",
    "    a_slice_prev -- 3D slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters (filter) of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameter, scalar or of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- Scalar result of convolving the filter on the input slice\n",
    "    \"\"\"\n",
    "    \n",
    "    # Element-wise multiplication and summing up the result\n",
    "    Z = np.sum(a_slice_prev * W) + b.item()  # Use .item() to extract the single value from the array\n",
    "    \n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c6ad895-9140-4648-8b13-22e1d3bbf887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z =-6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Generate a random 3D slice of input data and filter (weights) of the same shape\n",
    "a_slice_prev = np.random.randn(4, 4, 3)  # Input data slice of shape (4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)             # Filter of shape (4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)             # Bias, a scalar\n",
    "\n",
    "# Apply the convolutional single-step function to the data slice\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "\n",
    "# Output the result of the convolution\n",
    "print(f\"Z ={Z}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e01576-3bee-49b0-a687-1d43f778bafe",
   "metadata": {},
   "source": [
    "### 3.3 - Convolutional Neural Networks - Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2e4ba1-98c0-4234-b236-7ec61af17af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Perform forward propagation for a convolutional layer.\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Output activations from the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Filters, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- Dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- Convolution output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- Cache of values for use in backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract dimensions\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, _, _, n_C) = W.shape\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    stride, pad = hparameters[\"stride\"], hparameters[\"pad\"]\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    n_H = (n_H_prev + 2 * pad - f) // stride + 1\n",
    "    n_W = (n_W_prev + 2 * pad - f) // stride + 1\n",
    "    \n",
    "    # Initialize output volume Z with zeros\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Pad the input volume\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(m):  # Iterate over the batch\n",
    "        a_prev_pad = A_prev_pad[i]  # Padded activation for current example\n",
    "        for h in range(n_H):        # Iterate over vertical axis\n",
    "            vert_start, vert_end = h * stride, h * stride + f\n",
    "            \n",
    "            for w in range(n_W):    # Iterate over horizontal axis\n",
    "                horiz_start, horiz_end = w * stride, w * stride + f\n",
    "                \n",
    "                for c in range(n_C):  # Iterate over channels (filters)\n",
    "                    # Slice the input and apply convolution with filter W and bias b\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    weights, bias = W[..., c], b[..., c]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, bias)\n",
    "    \n",
    "    # Ensure output shape is as expected\n",
    "    assert Z.shape == (m, n_H, n_W, n_C)\n",
    "    \n",
    "    # Store values in cache for backward propagation\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7639ec6-788c-4688-a3ef-cbe89ccbc898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Z =0.6923608807576933\n",
      "Z[3,2,1] =[-1.28912231  2.27650251  6.61941931  0.95527176  8.25132576  2.31329639\n",
      " 13.00689405  2.34576051]\n",
      "cache_conv[0][1][2][3] =[-1.1191154   1.9560789  -0.3264995  -1.34267579]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Generate random input data, weights, and biases\n",
    "A_prev = np.random.randn(10, 5, 7, 4)  # Input activations (batch size: 10, height: 5, width: 7, channels: 4)\n",
    "W = np.random.randn(3, 3, 4, 8)        # Filters (height: 3, width: 3, input channels: 4, output channels: 8)\n",
    "b = np.random.randn(1, 1, 1, 8)        # Biases (1 for each output channel)\n",
    "\n",
    "# Define hyperparameters for padding and stride\n",
    "hparameters = {\"pad\": 1, \"stride\": 2}\n",
    "\n",
    "# Perform the forward convolution operation\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Print summary statistics and sample values for verification\n",
    "print(f\"Mean of Z ={np.mean(Z)}\")          # Mean of the output volume\n",
    "print(f\"Z[3,2,1] ={Z[3, 2, 1]}\")           # Specific output element for debugging\n",
    "print(f\"cache_conv[0][1][2][3] ={cache_conv[0][1][2][3]}\")  # Cached activation sample for backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109c0b7-c22f-484e-940f-0ae546519983",
   "metadata": {},
   "source": [
    "## 4 - Pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42652b3-938b-4d95-be9a-e1aba3c2b1d8",
   "metadata": {},
   "source": [
    "### 4.1 - Forward Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5d6ac8a-b368-4cae-8421-7d7c7fb17426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer.\n",
    "\n",
    "    Parameters:\n",
    "    A_prev (numpy.ndarray): Input data of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters (dict): Dictionary containing \"f\" (filter size) and \"stride\"\n",
    "    mode (str): Pooling mode to apply, either \"max\" or \"average\"\n",
    "\n",
    "    Returns:\n",
    "    A (numpy.ndarray): Output of the pool layer, with shape (m, n_H, n_W, n_C)\n",
    "    cache (tuple): Contains input and hparameters for use in backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions and hyperparameters\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "\n",
    "    # Define output dimensions\n",
    "    n_H = 1 + (n_H_prev - f) // stride\n",
    "    n_W = 1 + (n_W_prev - f) // stride\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    # Perform pooling operation\n",
    "    for i in range(m):                         # loop over training examples\n",
    "        for h in range(n_H):                   # loop over height of output\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            for w in range(n_W):               # loop over width of output\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                for c in range(n_C):           # loop over channels\n",
    "                    # Select the slice for pooling\n",
    "                    a_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Apply pooling operation based on mode\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_slice)\n",
    "    \n",
    "    # Cache values for backpropagation\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Ensure output shape is correct\n",
    "    assert A.shape == (m, n_H, n_W, n_C)\n",
    "    \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14616f72-e501-40c2-84be-a7e071b55cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape =(2, 3, 3, 3)\n",
      "A =\n",
      "[[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.46210794 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.14472371 0.90159072 2.10025514]\n",
      "   [1.14472371 0.90159072 1.65980218]\n",
      "   [1.14472371 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 0.84616065 0.82797464]\n",
      "   [0.69803203 0.84616065 1.2245077 ]\n",
      "   [0.69803203 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.84616065 1.27375593]\n",
      "   [1.96710175 0.84616065 1.23616403]\n",
      "   [1.62765075 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.86888616 1.27375593]\n",
      "   [1.96710175 0.86888616 1.23616403]\n",
      "   [1.62765075 1.12141771 0.79280687]]]]\n",
      "\n",
      "mode = average\n",
      "A.shape =(2, 3, 3, 3)\n",
      "A =\n",
      "[[[[-3.01046719e-02 -3.24021315e-03 -3.36298859e-01]\n",
      "   [ 1.43310483e-01  1.93146751e-01 -4.44905196e-01]\n",
      "   [ 1.28934436e-01  2.22428468e-01  1.25067597e-01]]\n",
      "\n",
      "  [[-3.81801899e-01  1.59993515e-02  1.70562706e-01]\n",
      "   [ 4.73707165e-02  2.59244658e-02  9.20338402e-02]\n",
      "   [ 3.97048605e-02  1.57189094e-01  3.45302489e-01]]\n",
      "\n",
      "  [[-3.82680519e-01  2.32579951e-01  6.25997903e-01]\n",
      "   [-2.47157416e-01 -3.48524998e-04  3.50539717e-01]\n",
      "   [-9.52551510e-02  2.68511000e-01  4.66056368e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.73134159e-01  3.23771981e-01 -3.43175716e-01]\n",
      "   [ 3.80634669e-02  7.26706274e-02 -2.30268958e-01]\n",
      "   [ 2.03009393e-02  1.41414785e-01 -1.23158476e-02]]\n",
      "\n",
      "  [[ 4.44976963e-01 -2.61694592e-03 -3.10403073e-01]\n",
      "   [ 5.08114737e-01 -2.34937338e-01 -2.39611830e-01]\n",
      "   [ 1.18726772e-01  1.72552294e-01 -2.21121966e-01]]\n",
      "\n",
      "  [[ 4.29449255e-01  8.44699612e-02 -2.72909051e-01]\n",
      "   [ 6.76351685e-01 -1.20138225e-01 -2.44076712e-01]\n",
      "   [ 1.50774518e-01  2.89111751e-01  1.23238536e-03]]]]\n"
     ]
    }
   ],
   "source": [
    "#### Stride = 1\n",
    "\n",
    "# Set up test case parameters\n",
    "np.random.seed(1)  # Seed for reproducibility\n",
    "A_prev = np.random.randn(2, 5, 5, 3)  # Sample input with shape (m=2, height=5, width=5, channels=3)\n",
    "hparameters = {\"stride\": 1, \"f\": 3}  # Pooling parameters: stride and filter size\n",
    "\n",
    "# Test max pooling mode\n",
    "print(\"mode = max\")\n",
    "A_max, _ = pool_forward(A_prev, hparameters, mode=\"max\")\n",
    "print(f\"A.shape ={A_max.shape}\")  # Expected output shape\n",
    "print(f\"A =\\n{A_max}\")  # Output of max pooling\n",
    "\n",
    "print()  # Blank line for separation\n",
    "\n",
    "# Test average pooling mode\n",
    "print(\"mode = average\")\n",
    "A_avg, _ = pool_forward(A_prev, hparameters, mode=\"average\")\n",
    "print(f\"A.shape ={A_avg.shape}\")  # Expected output shape\n",
    "print(f\"A =\\n{A_avg}\")  # Output of average pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fa776aa-37dc-4cf4-b3a7-7db779ea10fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape =(2, 2, 2, 3)\n",
      "A =\n",
      "[[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 0.84616065 0.82797464]\n",
      "   [0.69803203 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.86888616 1.27375593]\n",
      "   [1.62765075 1.12141771 0.79280687]]]]\n",
      "\n",
      "mode = average\n",
      "A.shape =(2, 2, 2, 3)\n",
      "A =\n",
      "[[[[-0.03010467 -0.00324021 -0.33629886]\n",
      "   [ 0.12893444  0.22242847  0.1250676 ]]\n",
      "\n",
      "  [[-0.38268052  0.23257995  0.6259979 ]\n",
      "   [-0.09525515  0.268511    0.46605637]]]\n",
      "\n",
      "\n",
      " [[[-0.17313416  0.32377198 -0.34317572]\n",
      "   [ 0.02030094  0.14141479 -0.01231585]]\n",
      "\n",
      "  [[ 0.42944926  0.08446996 -0.27290905]\n",
      "   [ 0.15077452  0.28911175  0.00123239]]]]\n"
     ]
    }
   ],
   "source": [
    "#### Stride = 2\n",
    "\n",
    "# Set up test case parameters\n",
    "np.random.seed(1)  # Seed for reproducibility\n",
    "A_prev = np.random.randn(2, 5, 5, 3)  # Sample input with shape (m=2, height=5, width=5, channels=3)\n",
    "hparameters = {\"stride\": 2, \"f\": 3}  # Pooling parameters: stride and filter size\n",
    "\n",
    "# Test max pooling mode\n",
    "print(\"mode = max\")\n",
    "A_max, _ = pool_forward(A_prev, hparameters, mode=\"max\")\n",
    "print(f\"A.shape ={A_max.shape}\")  # Expected output shape\n",
    "print(f\"A =\\n{A_max}\")  # Output of max pooling\n",
    "\n",
    "print()  # Blank line for separation\n",
    "\n",
    "# Test average pooling mode\n",
    "print(\"mode = average\")\n",
    "A_avg, _ = pool_forward(A_prev, hparameters, mode=\"average\")\n",
    "print(f\"A.shape ={A_avg.shape}\")  # Expected output shape\n",
    "print(f\"A =\\n{A_avg}\")  # Output of average pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77876528-9f79-41f7-aead-1bbb4afb3b2e",
   "metadata": {},
   "source": [
    "## 5 - Backpropagation in convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e64afd-fc3f-4b8a-b8e5-955bd02a982d",
   "metadata": {},
   "source": [
    "### 5.1 - Convolutional layer backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac2c10-8337-4164-85d8-55b516c6316f",
   "metadata": {},
   "source": [
    "#### 5.1.1 - Computing dA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f340a-2510-4b38-b40d-d738ac048461",
   "metadata": {},
   "source": [
    "#### 5.1.2 - Computing dW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac3b315-41e8-4f78-9341-15032ee2d1cb",
   "metadata": {},
   "source": [
    "#### 5.1.3 - Computing db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93a986e1-552f-454e-a70a-27635bd474b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements backward propagation for a convolutional layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the output of the conv layer (Z),\n",
    "          numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- Cache of values from the forward pass, containing (A_prev, W, b, hparameters)\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- Gradient of the cost with respect to the weights of the conv layer (W),\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- Gradient of the cost with respect to the biases of the conv layer (b),\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve values from the cache\n",
    "    A_prev, W, b, hparameters = cache\n",
    "    stride, pad = hparameters[\"stride\"], hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize gradients with zeros\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    dW = np.zeros_like(W)\n",
    "    db = np.zeros_like(b)\n",
    "    \n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = np.pad(A_prev, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode='constant', constant_values=0)\n",
    "    dA_prev_pad = np.pad(dA_prev, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode='constant', constant_values=0)\n",
    "    \n",
    "    # Loop over each training example\n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]  # ith example of padded A_prev\n",
    "        da_prev_pad = dA_prev_pad[i]  # ith example of padded dA_prev\n",
    "        \n",
    "        # Loop over output dimensions\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    # Define slice corners\n",
    "                    vert_start, vert_end = h * stride, h * stride + f\n",
    "                    horiz_start, horiz_end = w * stride, w * stride + f\n",
    "                    \n",
    "                    # Define slice and compute gradients\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dZ[i, h, w, c]\n",
    "                    dW[:, :, :, c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:, :, :, c] += dZ[i, h, w, c]\n",
    "        \n",
    "        # Unpad the gradient to get dA_prev for the ith example\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    # Check output shape\n",
    "    assert dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    \n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dae2deb6-f55b-44e0-abea-0b20d9c29c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n"
     ]
    }
   ],
   "source": [
    "# Initialize inputs and parameters for conv_forward\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)  # Example input with shape (m, height, width, channels)\n",
    "W = np.random.randn(2, 2, 3, 8)        # Weights with shape (filter_height, filter_width, input_channels, output_channels)\n",
    "b = np.random.randn(1, 1, 1, 8)        # Biases for each output channel\n",
    "hparameters = {\"pad\": 2, \"stride\": 2}  # Padding and stride hyperparameters\n",
    "\n",
    "# Run forward pass to obtain Z and cache\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Test backward pass with the obtained cache\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "\n",
    "# Output the means of gradients for quick verification\n",
    "print(f\"dA_mean = {np.mean(dA)}\")\n",
    "print(f\"dW_mean = {np.mean(dW)}\")\n",
    "print(f\"db_mean = {np.mean(db)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdcf72-f64b-49fd-affe-5a6c4d7eca22",
   "metadata": {},
   "source": [
    "### 5.2 - Pooling layer - backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdd68f-9790-4cc5-992e-faf408d129b0",
   "metadata": {},
   "source": [
    "#### 5.2.1 - Max pooling - backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a060b84-f151-4ef0-b90a-292ee8dd8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix `x` to identify the maximum entry in `x`.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- 2D NumPy array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Boolean array of the same shape as `x`, containing True at the position of the maximum entry in `x`, False elsewhere.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a mask with True at the max value's position in `x` and False elsewhere\n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "075bf86d-08ed-4594-87b1-03064e1f0bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =\n",
      "[[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =\n",
      "[[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "# Seed the random number generator for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Generate a 2x3 matrix of random values from a normal distribution\n",
    "x = np.random.randn(2, 3)\n",
    "\n",
    "# Apply the create_mask_from_window function to get a mask for the max entry in `x`\n",
    "mask = create_mask_from_window(x)\n",
    "\n",
    "# Display the matrix `x` and the generated mask\n",
    "print(f'x =\\n{x}')\n",
    "print(f\"mask =\\n{mask}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e12ccea-347f-426a-889d-4d98874dd07f",
   "metadata": {},
   "source": [
    "#### 5.2.2 - Average pooling - backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b7209c2-8b47-46aa-91f5-7dac1643a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes a scalar value across a matrix of a specified shape.\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- Scalar to be distributed across the output matrix.\n",
    "    shape -- Tuple (n_H, n_W) representing the dimensions of the output matrix.\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of shape (n_H, n_W) where each element is the distributed value of dz.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack dimensions from the provided shape\n",
    "    n_H, n_W = shape\n",
    "    \n",
    "    # Calculate the distributed value by dividing dz across all elements in the matrix\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix filled with the calculated average value\n",
    "    a = np.full((n_H, n_W), average)\n",
    "    \n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68bb0f1a-84e0-4ab8-b998-c624becb9e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed value =\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "a = distribute_value(2, (2, 2))\n",
    "print(f'Distributed value =\\n{a}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b991c-56b8-48eb-a1ba-e9af2e3abf02",
   "metadata": {},
   "source": [
    "#### 5.2.3 Pulling it together: Pooling backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31d07ed6-1a1b-435e-848a-61811e1bac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pool_backward(dA, cache, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for a pooling layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- Gradient of the cost with respect to the output of the pooling layer, same shape as the output (A)\n",
    "    cache -- Cache output from the forward pass of the pooling layer, containing the input and hyperparameters\n",
    "    mode -- Pooling mode (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the input of the pooling layer, same shape as the input (A_prev)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack cache values\n",
    "    A_prev, hparameters = cache\n",
    "    stride, f = hparameters[\"stride\"], hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from dA and A_prev\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    _, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize the gradient for the input of the pooling layer\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    for i in range(m):  # Iterate over training examples\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):  # Iterate over the output height\n",
    "            for w in range(n_W):  # Iterate over the output width\n",
    "                for c in range(n_C):  # Iterate over channels\n",
    "                    \n",
    "                    # Define corners of the slice\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Slice the region of interest in the input\n",
    "                    if mode == \"max\":\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
    "                    \n",
    "                    elif mode == \"average\":\n",
    "                        da = dA[i, h, w, c]\n",
    "                        shape = (f, f)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "    \n",
    "    # Ensure the shape matches the input\n",
    "    assert dA_prev.shape == A_prev.shape\n",
    "    \n",
    "    return dA_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0965c97f-6d41-4e42-975a-7be5929e9c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "Mean of dA: 0.14571390272918056\n",
      "dA_prev[1,1]: [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "Mean of dA: 0.14571390272918056\n",
      "dA_prev[1,1]: [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialize input data and hyperparameters for pooling\n",
    "A_prev = np.random.randn(5, 5, 3, 2)  # Input matrix with shape (5, 5, 3, 2)\n",
    "hparameters = {\"stride\": 1, \"f\": 2}  # Pooling hyperparameters\n",
    "\n",
    "# Perform forward pooling operation\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "\n",
    "# Generate a random gradient matrix with the same shape as A for testing backpropagation\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "# Backpropagation for max pooling\n",
    "dA_prev = pool_backward(dA, cache, mode=\"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"Mean of dA:\", np.mean(dA))\n",
    "print(\"dA_prev[1,1]:\", dA_prev[1,1])  \n",
    "print()  # Blank line for separation in output\n",
    "\n",
    "# Backpropagation for average pooling\n",
    "dA_prev = pool_backward(dA, cache, mode=\"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"Mean of dA:\", np.mean(dA))\n",
    "print(\"dA_prev[1,1]:\", dA_prev[1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa667b18-650c-441f-9baf-fecc6570336e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
